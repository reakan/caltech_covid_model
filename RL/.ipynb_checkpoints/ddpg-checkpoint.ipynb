{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import modelutils_v2 as modelutils\n",
    "import copy,random\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input (at least the state) into the network is 11650 elements long. The action vector is npeople long. The critic network maps the state and action vectors to a scalar. The actor network maps a state to an action. The action output will be the theta of a bernoulli trial to determine if an individual will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "npeople = 100\n",
    "inputlen = 11650+npeople #we'll need to change the 11650 is the underlying model changes\n",
    "nlocations=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.d1 = Dense(inputlen+npeople, activation='relu')\n",
    "    self.d2 = Dense(12000,activation='relu')\n",
    "    self.d3 = Dense(5000,activation='relu')\n",
    "    self.d4 = Dense(1000,activation='relu')\n",
    "    self.d5 = Dense(500,activation='relu')\n",
    "    self.d6 = Dense(100,activation='relu')\n",
    "    self.dout = Dense(1,activation='relu')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.d1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.d3(x)\n",
    "    x = self.d4(x)\n",
    "    x = self.d5(x)\n",
    "    x = self.d6(x)\n",
    "    return self.dout(x)\n",
    "\n",
    "class Actor(Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.d1 = Dense(inputlen, activation='relu')\n",
    "    self.d2 = Dense(12000,activation='relu')\n",
    "    self.d2 = Dense(12000,activation='relu')\n",
    "    self.d3 = Dense(5000,activation='relu')\n",
    "    self.d4 = Dense(1000,activation='relu')\n",
    "    self.d5 = Dense(500,activation='relu')\n",
    "    self.d6 = Dense(250,activation='relu')\n",
    "    self.dout = Dense(npeople,activation='softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.d1(x)\n",
    "    x = self.d2(x)\n",
    "    x = self.d3(x)\n",
    "    x = self.d4(x)\n",
    "    x = self.d5(x)\n",
    "    x = self.d6(x)\n",
    "    return self.dout(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "critic_target = Critic()\n",
    "critic_raw = Critic()\n",
    "\n",
    "actor_target = Actor()\n",
    "actor_raw = Actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 9\n",
      "1 / 9\n",
      "2 / 9\n",
      "3 / 9\n",
      "4 / 9\n",
      "5 / 9\n",
      "6 / 9\n",
      "7 / 9\n",
      "8 / 9\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "bufferlen = 10\n",
    "batchsize = 8\n",
    "gamma=1e-3\n",
    "tau = 1e-1\n",
    "\n",
    "homelocs = np.zeros([nlocations,npeople])\n",
    "for person_idx in range(npeople):\n",
    "    homelocs[np.random.choice(np.arange(nlocations)),person_idx]=1.\n",
    "\n",
    "init_state = modelutils.state(homelocs) \n",
    "\n",
    "rand_action[rand_action>0.2] = 999\n",
    "rand_action[rand_action<0.2] = 1\n",
    "rand_action[rand_action==999] = 0\n",
    "\n",
    "next_state = init_state.update_state(rand_action)\n",
    "\n",
    "replay_buffer = [(init_state,rand_action,next_state.reward(),next_state)]\n",
    "\n",
    "for _ in range(bufferlen-1):\n",
    "    print(_,'/',bufferlen-1)\n",
    "    init_state = replay_buffer[-1][-1]\n",
    "    \n",
    "    rand_action = st.uniform.rvs(size=npeople)\n",
    "    rand_action[rand_action>0.2] = 999\n",
    "    rand_action[rand_action<0.2] = 1\n",
    "    rand_action[rand_action==999] = 0\n",
    "    \n",
    "    next_state = init_state.update_state(rand_action)\n",
    "    \n",
    "    replay_buffer.append((init_state,rand_action,next_state.reward(),next_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(replay_buffer):\n",
    "  with tf.GradientTape() as tape:\n",
    "    \n",
    "    #create a new sars tuple based on the selected action\n",
    "    \n",
    "    init_state = replay_buffer[-1][-1]\n",
    "    actor_network_input = init_state.flatten()\n",
    "    \n",
    "    sampled_action = actor_raw(actor_network_input, training=True)  #this action vector contains probabilities, which we need to convert into a binary format a la bernoulli trials\n",
    "    binary_action = [1 if st.uniform.rvs()<x else 0 for x in sampled_action ]\n",
    "    \n",
    "    next_state = init_state.update_state(binary_action)\n",
    "    replay_buffer.append((init_state,rand_action,next_state.reward(),next_state))\n",
    "    replay_buffer.pop()  #need to do this or else the buffer will grow endlessly\n",
    "    \n",
    "    #grab a minibatch from the replay buffer for training\n",
    "    training_minibatch = random.sample(replay_buffer,batchsize)\n",
    "    \n",
    "    #we need to manually calculate yi for each item in the batch so we have something to train our critic network. Output needs to be an array as that's what's returned from tensorflow.\n",
    "    #yi = ri + gamma*q'(s_{i+1},u'(s_{i+1}))\n",
    "    #we'll start by calculating a list of actions from the target actor network\n",
    "    statelist = [training_minibatch[x][-1] for x in range(len(training_minibatch))]\n",
    "    rewardlist = np.array([training_minibatch[x][2] for x in range(len(training_minibatch))]) #cast immediately to array as its simply a list of scalars\n",
    "    \n",
    "    action_list = [actor_target(x) for x in statelist]\n",
    "    binary_action_list = [np.array([1 if st.uniform.rvs()<x else 0 for x in sampled_action]) for sampled_action in action_list]  #you really need to check these two lines\n",
    "    \n",
    "    #next, we'll feed these actions into the target critic network to get a list of scalar values\n",
    "    value_list = np.array([gamma * critic_target(np.concatenate([sampled_state.flatten(),sampled_action])) for sampled_state,sampled_action in zip(statelist,binary_action_list)])\n",
    "    \n",
    "    #we can finally assemble yi using the rewards\n",
    "    yi = np.array(rewardlist + value_list)\n",
    "    \n",
    "    #onto defining the loss for the critic network. first we need to assemble the input for the critic_raw network\n",
    "    prev_statelist = [training_minibatch[x][0] for x in range(len(training_minibatch))]\n",
    "    prev_actionlist = [training_minibatch[x][1] for x in range(len(training_minibatch))]\n",
    "    prev_binary_action_list = [np.array([1 if st.uniform.rvs()<x else 0 for x in sampled_action]) for sampled_action in prev_actionlist]  #you really need to check these two lines\n",
    "    \n",
    "    critic_raw_output = np.array([gamma * critic_raw(np.concatenate([sampled_state.flatten(),sampled_action])) for sampled_state,sampled_action in zip(prev_statelist,prev_binary_action_list)])\n",
    "    \n",
    "    \n",
    "    critic_loss = tf.keras.losses.MeanSquaredError(yi,critic_raw_output)\n",
    "    \n",
    "    \n",
    "    #loss = loss_object(labels, predictions)\n",
    "  #gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  #optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  #train_loss(loss)\n",
    "  #train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main training loop\n",
    "for episode_idx in tqdm(range(episodes)):\n",
    "\n",
    "\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "\n",
    "\n",
    "  template = 'Episode {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print(template.format(episode_idx + 1,\n",
    "                        train_loss.result(),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
